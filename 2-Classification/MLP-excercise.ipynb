{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Multilayer Perceptron\n",
        "\n[Similar tutorial](https://www.tensorflow.org/get_started/mnist/pros)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "cXBoqDwV2z3V",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "biXxnT2wRQpo",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST dataset can be automatically downloaded via tensorflow with the following:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "-R0HUxwNHvI6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "56b00892-486c-4759-d82c-6ef8003164a9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1510073056333,
          "user_tz": -60,
          "elapsed": 947,
          "user": {
            "displayName": "David Nagy",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "116820342271745675144"
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try plotting one of the datapoints (from mnist.train.images) with matplotlib's `plt.imshow`. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "pG9Gd1JIRf0z",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define placeholders for the data and labels, however now instead of feeding in one datapoint at a time, define the placeholders for minibatches of size 200."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define a two-layer perceptron with a third output layer, where each layer is a linear transformation followed by a sigmoid nonlinearity:\n",
        "\n",
        "$$y=\\sigma_{softmax}\\circ A_2 \\circ \\sigma \\circ A_1\\circ \\mathbf{W} $$\n",
        "\n",
        "where \n",
        "- $A_i(x)=\\mathbf{W}_i \\mathbf{x}+\\mathbf{b}_i$,\n",
        "- $\\sigma$ is a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function (`tf.nn.sigmoid`) and\n",
        "- $\\sigma_{softmax}$ is a [softmax](https://en.wikipedia.org/wiki/Softmax_function) function (`tf.nn.softmax`).\n",
        "\nAs initial values for the parameters, use a truncated normal with `stddev = 0.1` for the weight and a constant `0.1` for the bias."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "8k9dah0HRh08",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a cross-entropy loss function \n",
        "$$-\\sum y_{true} \\cdot \\log y$$\n",
        "and an optimizer. The loss should be minimised on average over the minibatch. Try both the gradient descent optimizer from the previous excercise and the [Adam](https://arxiv.org/abs/1412.6980) optimizer. There is [possible numerical instability](https://deepnotes.io/softmax-crossentropy) of the loss function if it is defined directly with the above equation, which can be cirumvented for example by using the built-in `tf.nn.softmax_cross_entropy_with_logits` in the loss."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "DWhv9Tj0RjNJ",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, write a training loop where you initialize the variables and iterate over minibatches of data. You can get the next minibatch of size N with:\n",
        "```\n",
        "x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "```\n",
        "Print out the loss, train and test errors at every n-th step, either to stdout or to tensorboard. As a guide, test accuracy of at least 97% is possible with this setup."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Multilayer perceptron.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.4.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}